# ML Math Notes

Math foundations for Machine Learning, focusing on **intuition, visualization, and practical understanding** rather than heavy proofs.

Topics covered include:
- Linear Algebra
- Calculus
- Probability & Statistics
- Optimization

This repository is built progressively with notebooks and visual explanations that directly connect math concepts to how ML models work.

---

## Structure & Progress

### Book 1 — Vectors (Data Representation)
- Vector intuition
- Data points as vectors
- Vector magnitude and distance
- ML view: samples and features in vector space

### Book 2 — Matrices & Dot Product (Prediction Mechanism)
- Matrix shapes and intuition
- Matrix–vector multiplication
- Dot product
- ML view: linear model predictions as weighted sums

### Book 3 — Derivatives (Learning Signal)
- Derivative as rate of change
- Slope visualization
- Sensitivity of loss
- ML view: how the model knows it is wrong

### Book 4 — Gradients & Gradient Descent (Optimization)
- Partial derivatives
- Gradient as a vector of slopes
- Direction of steepest descent
- Numerical gradient computation
- ML view: updating all weights simultaneously to minimize loss

### Book 5 — Probability & Statistics (Data Behavior)
- Mean, variance, and standard deviation
- Data distribution and spread
- Noise intuition
- ML view: why data is noisy and why loss cannot be zero

---

## Learning Philosophy

- Math is treated as a **tool**, not the goal
- Focus on **ML relevance and intuition**
- Concepts are implemented and visualized in NumPy
- Heavy theory and proofs are intentionally avoided

This repository serves as a foundation for:
- Machine Learning from scratch
- Model training intuition
- Understanding optimization and loss minimization

